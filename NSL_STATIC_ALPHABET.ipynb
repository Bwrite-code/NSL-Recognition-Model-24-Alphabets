{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Xu1wHo0ETbo2aftFmGPFTrJMPxcQ3cgn","authorship_tag":"ABX9TyMjY2+78L+kla0zl25SqRze"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**DEVELOP A MODEL FOR RECOGNIZING 24 STATIC ALPHABETS OF THE NIGERIAN SIGN LANGUAGE (NSL)**"],"metadata":{"id":"JDdacuCm02ol"}},{"cell_type":"markdown","source":["#Data Preprocessing\n","\n","**Normalize and Resize Images**\n","\n","Images resized to a uniform size (64x64) and normalized"],"metadata":{"id":"mkyfC5okgcrG"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import os\n","\n","# Function to preprocess an image\n","def preprocess_image(image_path, target_size=(64, 64)):\n","    # Load image\n","    img = cv2.imread(image_path)\n","\n","    # Resize image to target size\n","    img_resized = cv2.resize(img, target_size)\n","\n","    # Convert image to grayscale\n","    img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n","\n","    # Normalize pixel values to range [0, 1]\n","    img_normalized = img_gray / 255.0\n","\n","    return img_normalized\n","\n","# Example: Apply preprocessing to all images in the folder\n","image_folder = '/content/drive/MyDrive/Colab Notebooks/sign image corpuses'\n","processed_images = []\n","\n","for img_filename in os.listdir(image_folder):\n","    img_path = os.path.join(image_folder, img_filename)\n","\n","    if img_filename.endswith('.jpg') or img_filename.endswith('.png'):\n","        processed_img = preprocess_image(img_path)\n","        processed_images.append(processed_img)\n","\n","# Now 'processed_images' contains your normalized and resized images\n"],"metadata":{"id":"Tz1YA-EggrNh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Data Augmentation**\n","\n","Data augmentation applied to increase the variability of the dataset, which helps the model generalize better."],"metadata":{"id":"fHlHQn9oiAA2"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","\n","# Initialize the ImageDataGenerator for real-time data augmentation\n","datagen = ImageDataGenerator(\n","    rotation_range=40,       # Rotate by up to 30 degrees\n","    width_shift_range=0.2,   # Shift the image horizontally by up to 20%\n","    height_shift_range=0.2,  # Shift the image vertically by up to 20%\n","    shear_range=0.2,         # Apply shear transformation\n","    zoom_range=0.2,          # Zoom in by up to 20%\n","    horizontal_flip=True,    # Flip images horizontally\n","    fill_mode='nearest'      # Fill missing pixels after transformation\n",")\n","\n","\n","\n","\n","# Function to augment image (add channel dimension)\n","def augment_image(image):\n","    # Add batch and channel dimensions to the image\n","    image = np.expand_dims(image, axis=-1)  # Shape becomes (height, width, 1)\n","    image = np.expand_dims(image, axis=0)   # Shape becomes (1, height, width, 1)\n","\n","    # Generate augmented images\n","    augmented_images = datagen.flow(image, batch_size=1)\n","\n","    # Retrieve one augmented image\n","    augmented_image = next(augmented_images)[0]\n","\n","    return augmented_image\n","\n","# Example: Apply preprocessing and augmentation to all images in the folder\n","augmented_images = []\n","\n","for img in processed_images:\n","    augmented_img = augment_image(img)\n","    augmented_images.append(augmented_img)\n","\n","# Now 'augmented_images' contains your augmented images\n","\n","\n"],"metadata":{"id":"xWPYdTG6iKL4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Feature Extraction\n","\n","In this step, the relevant features from the preprocessed images are extracted. The goal is to obtain meaningful information that will help the model classify the images into one of the 24 static NSL alphabets. Four different feature extraction techniques will be used:\n","\n","    1. Histogram of Oriented Gradients (HOG)\n","    2. Local Binary Patterns (LBP)\n","    3. Edge Orientation Histogram (EOH)\n","    4. Speeded-Up Robust Features (SURF)"],"metadata":{"id":"0PzuJ54cnJQ4"}},{"cell_type":"markdown","source":["**Histogram of Oriented Gradients (HOG)**\n","\n","HOG is a feature descriptor that captures the edge directions and magnitudes of an image. It is commonly used in object detection and image classification tasks."],"metadata":{"id":"fS20algVnfq2"}},{"cell_type":"code","source":["from skimage.feature import hog\n","from skimage import exposure\n","import numpy as np\n","\n","# Function to compute HOG features for grayscale images\n","def extract_hog_features(image):\n","    # Compute HOG features (no need for multichannel argument since the image is grayscale)\n","    features, hog_image = hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n","\n","    # Rescale the HOG image for better visualization\n","    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n","\n","    return features\n","\n","# Example usage for one image\n","#hog_features = extract_hog_features(processed_images[0])  # process one image\n"],"metadata":{"id":"Xh5yoEaFnq_j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Local Binary Patterns (LBP)**\n"],"metadata":{"id":"YX3dnp8upz09"}},{"cell_type":"code","source":["from skimage.feature import local_binary_pattern\n","import numpy as np\n","\n","# Function to compute LBP features\n","def extract_lbp_features(image, radius=1, n_points=8):\n","    # Convert the image to uint8 type to avoid the warning\n","    image = (image * 255).astype(np.uint8)  # scale and convert to integer type\n","\n","    # Compute LBP features\n","    lbp = local_binary_pattern(image, n_points, radius, method='uniform')\n","\n","    # Calculate the LBP histogram\n","    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points+3), range=(0, n_points+2))\n","\n","    # Normalize the histogram\n","    lbp_hist = lbp_hist.astype('float')\n","    lbp_hist /= (lbp_hist.sum() + 1e-6)\n","\n","    return lbp_hist\n","\n","# Example usage for one image\n","#lbp_features = extract_lbp_features(processed_images[0])  # process one image\n"],"metadata":{"id":"o8Ax5Z4mqSpE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Edge Orientation Histogram (EOH)**\n","\n","EOH captures the gradient direction and magnitude, similar to HOG but focuses on the orientation of edges.\n","\n","Hereâ€™s the implementation for EOH:"],"metadata":{"id":"0W1TV-AurrjK"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","# Function to compute EOH features\n","def extract_eoh_features(image):\n","    # Compute gradient along x and y axes\n","    grad_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n","    grad_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n","\n","    # Compute the gradient orientation (angle)\n","    magnitude, angle = cv2.cartToPolar(grad_x, grad_y, angleInDegrees=True)\n","\n","    # Create the EOH histogram of orientations (18 bins)\n","    angle_hist = np.histogram(angle.ravel(), bins=18, range=(0, 180))[0]\n","\n","    # Normalize the histogram\n","    angle_hist = angle_hist.astype('float')\n","    angle_hist /= (angle_hist.sum() + 1e-6)\n","\n","    return angle_hist\n","\n","# Example usage for one image\n","#eoh_features = extract_eoh_features(processed_images[0])  # process one image\n"],"metadata":{"id":"xC-qeXlMr3hJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Combining the Features**\n","\n","Combining the HOG, LBP, and EOH features into one feature vector:"],"metadata":{"id":"PKDjwjuctqly"}},{"cell_type":"code","source":["# Function to concatenate HOG, LBP, and EOH features\n","def combine_features(hog_features, lbp_features, eoh_features):\n","    combined_features = np.hstack((hog_features, lbp_features, eoh_features))\n","    return combined_features\n","\n","# Example for one image\n","#combined_features = combine_features(hog_features, lbp_features, eoh_features)\n"],"metadata":{"id":"9j_iuXtVtuUN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Classification of the extracted features using a Support Vector Machine\n","\n","We'll use three different kernels: Linear Kernel, Gaussian Kernel (RBF), and Radial Basis Function Kernel.\n","\n","We will classify the features we extracted (HOG, LBP, and EOH) using an SVM classifier. We'll also use the One-vs-All approach to handle multi-class classification.\n"],"metadata":{"id":"JAzdE3n-t2kh"}},{"cell_type":"markdown","source":["**SVM Classification**\n"],"metadata":{"id":"VN23FiFLCocY"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","\n","# Folder containing the images\n","image_folder = '/content/drive/MyDrive/Colab Notebooks/sign image corpuses'  # Update this to your actual path\n","\n","# List of alphabet letters excluding 'j' and 'z' (a to y, minus j and z)\n","alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n","\n","# Initialize lists to store features and labels\n","combined_features = []  # This will store all extracted features\n","labels = []  # This will store the corresponding labels (0-23 for NSL alphabets)\n","\n","# Loop through the images in the folder\n","for image_name in os.listdir(image_folder):\n","    if image_name.endswith(\".png.PNG\"):  # Ensure we're only processing .png image files\n","\n","        # Extract the letter (a, b, c, ...) from the image name (e.g., 'a1.png' -> 'a')\n","        letter = image_name[0]  # First character of the filename is the letter (e.g., 'a' from 'a1.png')\n","\n","        # Check if the letter is valid (a to y excluding j and z)\n","        if letter in alphabet:\n","            # Determine the label from the alphabet list\n","            label_idx = alphabet.index(letter)  # Get the label index for the letter\n","\n","            # Construct the full path of the image\n","            image_path = os.path.join(image_folder, image_name)\n","\n","            # Read and process the image (convert to grayscale)\n","            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","            # Apply preprocessing steps like resizing, normalization here (if needed)\n","            image_resized = cv2.resize(image, (64, 64))  # Example of resizing to 64x64\n","\n","            # Extract HOG, LBP, and EOH features for each image\n","            hog_features = extract_hog_features(image_resized)\n","            lbp_features = extract_lbp_features(image_resized)\n","            eoh_features = extract_eoh_features(image_resized)\n","\n","            # Combine features\n","            combined_features.append(np.hstack((hog_features, lbp_features, eoh_features)))\n","\n","            # Add the corresponding label (0-23 based on alphabet)\n","            labels.append(label_idx)\n","\n","# Check the first few labels and corresponding letters\n","for i in range(min(10, len(labels))):  # Check up to the first 10 images, or fewer if there are less than 10 images\n","    print(f\"Image {i+1}: Label = {labels[i]}, Letter = {alphabet[labels[i]]}\")\n","\n","# Convert combined_features and labels to numpy arrays\n","combined_features = np.array(combined_features)\n","labels = np.array(labels)\n","\n","# Check the distribution of classes in the full dataset\n","print(\"\\nFull dataset labels distribution:\")\n","print(np.unique(labels, return_counts=True))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PerqcvpgCteC","executionInfo":{"status":"ok","timestamp":1735824554793,"user_tz":0,"elapsed":3071,"user":{"displayName":"Idowu Ibitayo Bright","userId":"06102306615109766612"}},"outputId":"20a42218-9965-4527-99f3-492b73816eb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Image 1: Label = 1, Letter = b\n","Image 2: Label = 2, Letter = c\n","Image 3: Label = 1, Letter = b\n","Image 4: Label = 2, Letter = c\n","Image 5: Label = 1, Letter = b\n","Image 6: Label = 1, Letter = b\n","Image 7: Label = 0, Letter = a\n","Image 8: Label = 0, Letter = a\n","Image 9: Label = 0, Letter = a\n","Image 10: Label = 0, Letter = a\n","\n","Full dataset labels distribution:\n","(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n","       17, 18, 19, 20, 21, 22, 23]), array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4,\n","       4, 4]))\n"]}]},{"cell_type":"markdown","source":["**Training SVM Classifier with Linear Kernel**"],"metadata":{"id":"E4Vy4PrjSt-Z"}},{"cell_type":"code","source":["#We'll use scikit-learn for implementing SVM linear kernel.\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Step 1: Split the dataset into training and testing sets using Stratified Sampling\n","# Adjust test_size to be at least the number of classes (24) divided by total samples (94)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    combined_features, labels, test_size=24/96, random_state=42, stratify=labels\n",")\n","\n","# Step 2: Normalize the features (scaling)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Step 3: Train the SVM Classifier with Linear Kernel\n","svm_linear = SVC(kernel='linear', random_state=42, C=1.0, gamma='scale')  # Tuning C and gamma\n","svm_linear.fit(X_train_scaled, y_train)\n","\n","# Step 4: Predict on the test set\n","y_pred = svm_linear.predict(X_test_scaled)\n","\n","# Step 5: Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Detailed evaluation: Classification report\n","print(\"\\nClassification Report:\")\n","# Use 'labels' parameter to ensure we specify all possible classes (0 to 23)\n","print(classification_report(y_test, y_pred, target_names=alphabet, labels=np.arange(24)))\n","\n","# Confusion Matrix\n","print(\"\\nConfusion Matrix:\")\n","# Use 'labels' parameter to ensure we specify all possible classes (0 to 23)\n","print(confusion_matrix(y_test, y_pred, labels=np.arange(24)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mekljRUun9x0","executionInfo":{"status":"ok","timestamp":1735824968575,"user_tz":0,"elapsed":387,"user":{"displayName":"Idowu Ibitayo Bright","userId":"06102306615109766612"}},"outputId":"5df5616d-05c5-4aed-ad5d-9203217d8694"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 12.50%\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           a       0.00      0.00      0.00         1\n","           b       0.00      0.00      0.00         1\n","           c       0.00      0.00      0.00         1\n","           d       0.00      0.00      0.00         1\n","           e       0.00      0.00      0.00         1\n","           f       0.00      0.00      0.00         1\n","           g       0.00      0.00      0.00         1\n","           h       0.00      0.00      0.00         1\n","           i       0.50      1.00      0.67         1\n","           k       0.00      0.00      0.00         1\n","           l       1.00      1.00      1.00         1\n","           m       0.00      0.00      0.00         1\n","           n       0.00      0.00      0.00         1\n","           o       0.00      0.00      0.00         1\n","           p       0.00      0.00      0.00         1\n","           q       0.00      0.00      0.00         1\n","           r       0.00      0.00      0.00         1\n","           s       0.00      0.00      0.00         1\n","           t       0.00      0.00      0.00         1\n","           u       0.00      0.00      0.00         1\n","           v       0.50      1.00      0.67         1\n","           w       0.00      0.00      0.00         1\n","           x       0.00      0.00      0.00         1\n","           y       0.00      0.00      0.00         1\n","\n","    accuracy                           0.12        24\n","   macro avg       0.08      0.12      0.10        24\n","weighted avg       0.08      0.12      0.10        24\n","\n","\n","Confusion Matrix:\n","[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"," [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"," [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n"," [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]},{"cell_type":"markdown","source":["**Training SVM Classifier with Radial Basis Function (RBF) Kernel**"],"metadata":{"id":"IR206L78TE5j"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","\n","\n","# Step 1: Split the dataset into training and testing sets using Stratified Sampling\n","X_train, X_test, y_train, y_test = train_test_split(\n","    combined_features, labels, test_size=24/96, random_state=42, stratify=labels\n",")\n","\n","# Step 2: Normalize the features (scaling)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Step 3: Train the SVM Classifier with RBF Kernel (non-linear kernel)\n","svm_rbf = SVC(kernel='rbf', random_state=42, C=1.0, gamma='scale')  # Using RBF kernel\n","svm_rbf.fit(X_train_scaled, y_train)\n","\n","# Step 4: Predict on the test set\n","y_pred = svm_rbf.predict(X_test_scaled)\n","\n","# Step 5: Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Detailed evaluation: Classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=alphabet, labels=np.arange(24)))\n","\n","# Confusion Matrix\n","print(\"\\nConfusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred, labels=np.arange(24)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lka3EDpJJpHZ","executionInfo":{"status":"ok","timestamp":1735823854698,"user_tz":0,"elapsed":326,"user":{"displayName":"Idowu Ibitayo Bright","userId":"06102306615109766612"}},"outputId":"e40208e4-51f4-46ff-f1af-c74015def3bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 4.17%\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           a       0.00      0.00      0.00         1\n","           b       0.00      0.00      0.00         1\n","           c       0.00      0.00      0.00         1\n","           d       0.00      0.00      0.00         1\n","           e       0.00      0.00      0.00         1\n","           f       0.00      0.00      0.00         1\n","           g       0.00      0.00      0.00         1\n","           h       0.00      0.00      0.00         1\n","           i       0.00      0.00      0.00         1\n","           k       0.00      0.00      0.00         1\n","           l       1.00      1.00      1.00         1\n","           m       0.00      0.00      0.00         1\n","           n       0.00      0.00      0.00         1\n","           o       0.00      0.00      0.00         1\n","           p       0.00      0.00      0.00         1\n","           q       0.00      0.00      0.00         1\n","           r       0.00      0.00      0.00         1\n","           s       0.00      0.00      0.00         1\n","           t       0.00      0.00      0.00         1\n","           u       0.00      0.00      0.00         1\n","           v       0.00      0.00      0.00         1\n","           w       0.00      0.00      0.00         1\n","           x       0.00      0.00      0.00         1\n","           y       0.00      0.00      0.00         1\n","\n","    accuracy                           0.04        24\n","   macro avg       0.04      0.04      0.04        24\n","weighted avg       0.04      0.04      0.04        24\n","\n","\n","Confusion Matrix:\n","[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"," [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"," [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"," [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"," [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]},{"cell_type":"markdown","source":["**Regularizing Radial Basis Function (RBF) to Improve Performance**"],"metadata":{"id":"JNQ3HaOKT4x0"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","# Set up the parameter grid for RBF kernel\n","param_grid = {\n","    'C': [0.1, 1, 10],\n","    'gamma': ['scale', 0.1, 1],\n","    'kernel': ['rbf']\n","}\n","\n","# Initialize and perform Grid Search with Cross-Validation\n","# Reduced cv to 3 to ensure it's less than the minimum samples per class\n","grid_search = GridSearchCV(SVC(), param_grid, cv=3)\n","grid_search.fit(X_train_scaled, y_train)\n","\n","print(f\"Best Parameters: {grid_search.best_params_}\")\n","y_pred_rbf = grid_search.predict(X_test_scaled)\n","print(f\"Accuracy with optimized RBF kernel: {accuracy_score(y_test, y_pred_rbf) * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G2othhAFQRKi","executionInfo":{"status":"ok","timestamp":1735825376387,"user_tz":0,"elapsed":349,"user":{"displayName":"Idowu Ibitayo Bright","userId":"06102306615109766612"}},"outputId":"fb1af65e-786f-43c0-8972-9e0308732593"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Best Parameters: {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n","Accuracy with optimized RBF kernel: 12.50%\n"]}]},{"cell_type":"markdown","source":["**Training SVM Classifier with Polynomial Kernel**"],"metadata":{"id":"ajy8XHFKUOHd"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","# Polynomial kernel SVM\n","svm_poly = SVC(kernel='poly', degree=3, C=1.0, gamma='scale', random_state=42)\n","svm_poly.fit(X_train_scaled, y_train)\n","y_pred_poly = svm_poly.predict(X_test_scaled)\n","\n","print(f\"Accuracy with Polynomial Kernel: {accuracy_score(y_test, y_pred_poly) * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pKmJ4qbPMVL","executionInfo":{"status":"ok","timestamp":1735825089022,"user_tz":0,"elapsed":337,"user":{"displayName":"Idowu Ibitayo Bright","userId":"06102306615109766612"}},"outputId":"54bf0187-3770-4005-a124-be2cce70de2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with Polynomial Kernel: 4.17%\n"]}]},{"cell_type":"markdown","source":["**Linear SVM with Principal Component Analysis (PCA)**"],"metadata":{"id":"QVOvhzdkUxEF"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","# Reduce dimensionality with PCA\n","pca = PCA(n_components=50)  # Adjust components as needed\n","X_train_pca = pca.fit_transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","\n","# Train the linear kernel SVM on reduced data\n","svm_linear = SVC(kernel='linear', random_state=42, C=1.0)\n","svm_linear.fit(X_train_pca, y_train)\n","y_pred_linear = svm_linear.predict(X_test_pca)\n","\n","print(f\"Accuracy with PCA + Linear Kernel: {accuracy_score(y_test, y_pred_linear) * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QujSpkjLPlUc","executionInfo":{"status":"ok","timestamp":1735825191464,"user_tz":0,"elapsed":1605,"user":{"displayName":"Idowu Ibitayo Bright","userId":"06102306615109766612"}},"outputId":"d7e9f496-faca-4f13-e99a-0a192e20d898"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with PCA + Linear Kernel: 8.33%\n"]}]},{"cell_type":"markdown","source":["**Training SVM with Sigmoid Kernel (Hyperbolic Tangent)**"],"metadata":{"id":"VBYiEcvLVkLv"}},{"cell_type":"code","source":["svm_sigmoid = SVC(kernel='sigmoid', C=1.0, gamma='scale', random_state=42)\n","svm_sigmoid.fit(X_train_scaled, y_train)\n","y_pred_sigmoid = svm_sigmoid.predict(X_test_scaled)\n","\n","print(f\"Accuracy with Sigmoid Kernel: {accuracy_score(y_test, y_pred_sigmoid) * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"My-mz7-5Qe7d","executionInfo":{"status":"ok","timestamp":1735825425505,"user_tz":0,"elapsed":350,"user":{"displayName":"Idowu Ibitayo Bright","userId":"06102306615109766612"}},"outputId":"dd59a86f-01c9-4b52-a0cf-5fec0debbbab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with Sigmoid Kernel: 4.17%\n"]}]}]}